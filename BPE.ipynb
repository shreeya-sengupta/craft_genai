{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc931d10-a7ad-4a1f-8c40-d38fa4add263",
   "metadata": {},
   "source": [
    "##### A simple implementation of the BPE algorithm from scratch. \n",
    "##### The objective is to understand the core idea of the algorithm, hence, a very basic limited implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b618cc-8235-480c-bf1a-b010ad645f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcb95f-9e89-44a8-bb2b-f1b4624430b7",
   "metadata": {},
   "source": [
    "# Implement BPE from Scratch\n",
    "#### step 1 - break down text into characters\n",
    "#### step 2 - calculate the frequency of bigrams\n",
    "#### step 3 - merge the most common pair to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0add15-0226-4d48-9a8f-4d2c22b3c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - break down text into characters\n",
    "\n",
    "def init_vocab(corpus):\n",
    "    # print(corpus)\n",
    "    input_text_with_special_char = [word + ' </w>' for word in corpus.split()]\n",
    "    # print(input_text_with_special_char)\n",
    "    vocab = []\n",
    "    for ip in input_text_with_special_char:\n",
    "        word_to_split = ' ' .join(list(ip.split()[0])) + \" </w>\"\n",
    "        vocab.append(word_to_split)\n",
    "    return vocab\n",
    "\n",
    "# input_text = \"Walker walked a long walk at someunknownbeach\"\n",
    "# print(init_vocab(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30e245a-23c9-4b70-97ee-014dfabd4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 - calculate the frequency of bigrams\n",
    "\n",
    "def bigram_frequency(vocab):\n",
    "    # print(f\"current vocab = {vocab}\")\n",
    "    vocab_len = len(vocab)\n",
    "    # print(f\"len of current vocab = {vocab_len}\")/\n",
    "    bigrams_freq = defaultdict(int)\n",
    "    for word in vocab:\n",
    "        word = word.split()\n",
    "        # print(word)\n",
    "        for i1 in range(len(word)-2):\n",
    "            pair = (word[i1],word[i1+1])\n",
    "            bigrams_freq[pair] += 1\n",
    "    # print(f\"frequency of bigrams = {bigrams_freq}\")\n",
    "    max_frequency = max(bigrams_freq.values())\n",
    "    # print(f\"max frequency = {most_frequent_pair}\")\n",
    "    merged_frequent_pair = [pair for pair in bigrams_freq if bigrams_freq[pair] == max_frequency][0]\n",
    "    # print(f\"most frequent pairs: {merged_frequent_pair}\")\n",
    "    return merged_frequent_pair, max_frequency\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a080ddc-2ac5-407c-be2a-28f0ea404abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 - merge the most common pair to the dictionary\n",
    "\n",
    "def merge_common_pair_update_vocab(vocab, merged_frequent_pair):\n",
    "    vocab_len = len(vocab)\n",
    "    # print(vocab)\n",
    "    # print(vocab_len)\n",
    "    most_frequent_pair_list = list(merged_frequent_pair)\n",
    "    # print(most_frequent_pair_list)\n",
    "    for cnt, word in enumerate(vocab):\n",
    "        # print(word)\n",
    "        word = word.split()\n",
    "        # print(word)\n",
    "        items_to_pop = list()\n",
    "        for y in range(len(word)-2):\n",
    "            # print(\"inside\")\n",
    "            # print(y, y+1)\n",
    "            if word[y] == most_frequent_pair_list[0] and word[y + 1] == most_frequent_pair_list[1]:\n",
    "                word[y] = most_frequent_pair_list[0] + most_frequent_pair_list[1]\n",
    "                # print(word[y])\n",
    "                items_to_pop.append(word[y + 1])\n",
    "            # print(f\"items to pop: {items_to_pop}\")\n",
    "        for item in items_to_pop:\n",
    "            word.remove(item)\n",
    "        # print(\"word = \", ' '.join(word))\n",
    "        vocab[cnt] = ' '.join(word)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8ef97-3169-4f60-828f-e314926b981e",
   "metadata": {},
   "source": [
    "### bpe orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95301aff-c145-4cfb-abc0-871e10f2f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_training():\n",
    "    input_text = \"Walker walked a long walk at someunknownbeach\"\n",
    "    corpus1 = init_vocab(input_text)\n",
    "    print(\"initial corpus = \", corpus1)\n",
    "    merges = 5\n",
    "    learned_merges = []\n",
    "    for i in range(merges):\n",
    "        mst_freq_pair, max_freq = bigram_frequency(corpus1)\n",
    "        # print(\"mst_freq_pair\", mst_freq_pair)\n",
    "        # if freq_gt_one:\n",
    "        corpus1 = merge_common_pair_update_vocab(corpus1, mst_freq_pair)\n",
    "        print(f\"merge: {i} ; updated corpus : {corpus1} \\n\")\n",
    "        learned_merges.append(mst_freq_pair)\n",
    "        # print(\"********************************************************** \\n\")\n",
    "    # final_corpus = \" \".join(corpus1)\n",
    "    print(f\"final vocab = {corpus1}\")\n",
    "    print(f\"learned merges: {learned_merges}\")\n",
    "    return corpus1, learned_merges\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3603151-e838-4b00-94eb-3bbab458b237",
   "metadata": {},
   "source": [
    "# use the learned merge to tokenise new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f71e5c-f206-4e83-bbb9-41fce8ca26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bpe(test_word, merges):\n",
    "    test_tokens = list(test_word) + ['</w>']\n",
    "    i = 0\n",
    "    while i < len(test_tokens) - 1:\n",
    "        pair = (test_tokens[i], test_tokens[i+1])\n",
    "        if pair in reversed(merges):\n",
    "            test_tokens[i] = ''.join(pair)\n",
    "            del test_tokens[i+1]\n",
    "            i = max(i-1, 0)  # re-check merged token with previous one\n",
    "        else:\n",
    "            i += 1\n",
    "    return test_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb92116-bfc9-41bd-8d1f-c1be1c7eb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial corpus =  ['W a l k e r </w>', 'w a l k e d </w>', 'a </w>', 'l o n g </w>', 'w a l k </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>']\n",
      "merge: 0 ; updated corpus : ['W al k e r </w>', 'w al k e d </w>', 'a </w>', 'l o n g </w>', 'w al k </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>'] \n",
      "\n",
      "merge: 1 ; updated corpus : ['W alk e r </w>', 'w alk e d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>'] \n",
      "\n",
      "merge: 2 ; updated corpus : ['W alke r </w>', 'w alke d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>'] \n",
      "\n",
      "merge: 3 ; updated corpus : ['Walke r </w>', 'w alke d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>'] \n",
      "\n",
      "merge: 4 ; updated corpus : ['Walker </w>', 'w alke d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>'] \n",
      "\n",
      "final vocab = ['Walker </w>', 'w alke d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>']\n",
      "learned merges: [('a', 'l'), ('al', 'k'), ('alk', 'e'), ('W', 'alke'), ('Walke', 'r')]\n"
     ]
    }
   ],
   "source": [
    "final_corpus, learned_merges = bpe_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801e87af-5277-48d7-b279-ccdb6fffcbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'alk', '</w>']\n"
     ]
    }
   ],
   "source": [
    "print(apply_bpe(\"walk\", learned_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cddf7f4-f3e5-410d-8647-6488069c1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['al', 'o', 'n', 'g', 'w', 'alk', '</w>']\n"
     ]
    }
   ],
   "source": [
    "print(apply_bpe(\"alongwalk\", learned_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d78b38a1-0485-4a4e-a884-d32fac360c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', ' ', 'l', 'o', 'n', 'g', ' ', 'w', 'alk', '</w>']\n"
     ]
    }
   ],
   "source": [
    "print(apply_bpe(\"a long walk\", learned_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035d43e-a09a-418a-a0fd-7910853c16d5",
   "metadata": {},
   "source": [
    "# tokens to embeddings - create embeddings\n",
    "#### assign token ids to the corpus plus some additional special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca6abbf-91db-4428-989f-62bda1d3de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_token_ids(corpus):\n",
    "    print(corpus)\n",
    "    token_to_id = {}\n",
    "    token_id = 1\n",
    "    for token in corpus:\n",
    "        print(token)\n",
    "        for tkn in token.split():\n",
    "            if tkn not in token_to_id.keys():\n",
    "                token_to_id[tkn] = token_id\n",
    "                token_id += 1\n",
    "        #Add some special tokens\n",
    "        token_to_id[\"<UNK>\"] = 400\n",
    "        token_to_id[\"<SPACE>\"] = 5000\n",
    "        token_to_id[\"</w>\"] = 6000\n",
    "    return token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "810bf6db-c831-4f88-abdf-a78e8c0edca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walker </w>', 'w alke d </w>', 'a </w>', 'l o n g </w>', 'w alk </w>', 'a t </w>', 's o m e u n k n o w n b e a c h </w>']\n",
      "Walker </w>\n",
      "w alke d </w>\n",
      "a </w>\n",
      "l o n g </w>\n",
      "w alk </w>\n",
      "a t </w>\n",
      "s o m e u n k n o w n b e a c h </w>\n",
      "{'Walker': 1, '</w>': 6000, '<UNK>': 400, '<SPACE>': 5000, 'w': 3, 'alke': 4, 'd': 5, 'a': 6, 'l': 7, 'o': 8, 'n': 9, 'g': 10, 'alk': 11, 't': 12, 's': 13, 'm': 14, 'e': 15, 'u': 16, 'k': 17, 'b': 18, 'c': 19, 'h': 20}\n"
     ]
    }
   ],
   "source": [
    "corpus_token_numbers = create_corpus_token_ids(final_corpus)\n",
    "print(corpus_token_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0ba1af-b3bd-4c08-8ef6-e6fd631d0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings for test words\n",
    "def encode(test_tkns, corpus_token_ids):\n",
    "    embeddings = []\n",
    "    # print(corpus_token_ids)\n",
    "    print(test_tkns)\n",
    "    for tkn_test in test_tkns:\n",
    "        # print(tkn_test)\n",
    "        if tkn_test in corpus_token_ids.keys():\n",
    "            embeddings.append(corpus_token_ids[tkn_test])\n",
    "        elif tkn_test == \" \":\n",
    "            embeddings.append(corpus_token_ids[\"<SPACE>\"])    \n",
    "        else:\n",
    "            embeddings.append(corpus_token_ids[\"<UNK>\"])\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c17489-27ce-4aa2-97cd-e2eea0bf887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'alk', '</w>']\n",
      "[3, 11, 6000]\n",
      "['al', 'o', 'n', 'g', 'w', 'alk', '</w>']\n",
      "[400, 8, 9, 10, 3, 11, 6000]\n",
      "['a', ' ', 'l', 'o', 'n', 'g', ' ', 'w', 'alk', '</w>']\n",
      "[6, 5000, 7, 8, 9, 10, 5000, 3, 11, 6000]\n"
     ]
    }
   ],
   "source": [
    "print(encode(['w', 'alk', '</w>'], corpus_token_numbers))\n",
    "print(encode(['al', 'o', 'n', 'g', 'w', 'alk', '</w>'], corpus_token_numbers))\n",
    "print(encode(['a', ' ', 'l', 'o', 'n', 'g', ' ', 'w', 'alk', '</w>'], corpus_token_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fe1db-944b-4607-b05d-4faee835bffe",
   "metadata": {},
   "source": [
    "# embeddings to tokens - decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b1e945e-eace-4804-b1c2-69b7a0be398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(embeddings, corpus_token_ids):\n",
    "    orig_tokens = []\n",
    "    for em in embeddings:\n",
    "        for key, value in corpus_token_ids.items():\n",
    "            if value == em:\n",
    "                if key == \"<SPACE>\":\n",
    "                    orig_tokens.append(' ')\n",
    "                else:\n",
    "                    orig_tokens.append(key)\n",
    "    orig_word = \"\".join(orig_tokens[:-1])\n",
    "    return orig_tokens, orig_word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d932cc5-6639-4b73-817f-649d6038c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded tokens :  ['w', 'alk', '</w>'] \n",
      "decoded word :  walk\n",
      "decoded tokens :  ['<UNK>', 'o', 'n', 'g', 'w', 'alk', '</w>'] \n",
      "decoded word :  <UNK>ongwalk\n",
      "decoded tokens :  ['a', ' ', 'l', 'o', 'n', 'g', ' ', 'w', 'alk', '</w>'] \n",
      "decoded word :  a long walk\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_word = decode([3, 11, 6000], corpus_token_numbers)\n",
    "print(\"decoded tokens : \", decoded_tokens, \"\\ndecoded word : \", decoded_word)\n",
    "\n",
    "decoded_tokens, decoded_word = decode([400, 8, 9, 10, 3, 11, 6000], corpus_token_numbers)\n",
    "print(\"decoded tokens : \", decoded_tokens, \"\\ndecoded word : \", decoded_word)\n",
    "\n",
    "decoded_tokens, decoded_word = decode([6, 5000, 7, 8, 9, 10, 5000, 3, 11, 6000], corpus_token_numbers)\n",
    "print(\"decoded tokens : \", decoded_tokens, \"\\ndecoded word : \", decoded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91482bc-b2de-457b-b5b3-014a15b9da32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
